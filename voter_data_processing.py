import pandas as pd
import numpy as np

def import_voter_data(voter_file_directory, geo_crosswalk_file):
	'''
	Imports voter roll data from TX Secretary of State record files, concatenates the files into a single DataFrame, performs memory optimization steps, then merges the result with a geographical crosswalk  file that allows for aggregation of voter records at various resolutions.

	Some of the memory optimization code and many of the concepts here
	are adapted from https://www.dataquest.io/blog/pandas-big-data/

	Parameters 
	----------
	voter_file_directory: filepath str. individual files (usually at county level) to import and then concatenate into a single master DataFrame. Expected filename convention is 'pir_*.txt'. This string should end with '/'.

	geo_crosswalk_file: filepath str. File that shares keys with the voter files, specifically the State County ID and Precinct Name fields. These will be used as a composite key for merging the two datasets and providing different geographical resolutions to the final output. This should point to a CSV file.


	Returns
	----------

	DataFrame of a similar format as those generated by individual files in the voter_file_directory, but with additional fields of Precinct ID, State House District, State Senate District, and US Congressional District. Note that this DataFrame has a column called _merge that indicates if merge of geography crosswalk data with voter rolls data was successful (with value of 'both') or unsuccessful (with value of 'left_only') for each record
	'''

	#These column names and field indices + widths copied from GitLab 
	#code, with an extra tuple added according
	#to TX Secretary of State (SOS) data fields definition file.
	
	#Define field names, start indices, and field widths
	fields = pd.read_csv('field_definitions.csv', index_col = 0)



	#Voter data column names
	cols = fields.columns.values



	#Row 2 of fields (iloc[1]) is the widths
	field_params = fields.iloc[1].tolist()

	#Create a generator for all data files in the voter rolls directory
	from glob import iglob
	all_files = iglob(voter_file_directory + 'pir_*.txt')

	#Build a complete DataFrame that includes every voter record in the data files
	counties = (pd.read_fwf(f, widths = field_params, header = None, 		names = cols) for f in all_files)
	all_voters_df = pd.concat(counties, ignore_index=True)

	#Typecast fields to optimize memory usage
	all_voters_df = all_voters_df.astype({'Precinct Name': 'str',
		'Suffix': 'category',
		'Gender': 'category',
		'Perm Designator': 'category',
		'Perm Directional Prefix': 'category',
		'Perm Street Type': 'category',
		'Perm Directional Suffix': 'category',
		'Perm Unit Type': 'category',
		'Perm City': 'category',
		'Perm ZIP Code': 'str',
		'Mailing City': 'category',
		'Mailing State': 'category',
		'Mailing ZIP Code': 'category',
		'Status Code': 'category',
		'Hispanic Surname Flag': 'category'})


	#Dates are of format YYYYMMDD
	all_voters_df['Date of Birth'] = pd.to_datetime(all_voters_df['Date of Birth'], 
		format = '%Y%m%d', errors = 'coerce')

	all_voters_df['EDR'] = pd.to_datetime(all_voters_df['EDR'],
		format = '%Y%m%d', errors = 'coerce')

	#Downcast numeric columns to save space
	for dtype in ['float', 'integer']:
		x = all_voters_df.select_dtypes(include = [dtype]).columns
		all_voters_df[x] = all_voters_df[x].apply(pd.to_numeric, 
			downcast = dtype)


	#--------------------------------------------------------------------------
	#Cleaning up Precinct Names

	def keep_numbers(matchObj):
		'''
		Makes sure that, when pd.Series.str.replace() is run to look for leading zeroes followed by
		a nonzero digit/symbol, only the zeros are replaced with whitespace. And, if the only thing present
		is a zero, then keep it
		'''
		
		return matchObj.group(0)[-1]


	#Cast to str type for all
	all_voters_df['Precinct Name'] = all_voters_df['Precinct Name'].astype('str')

	#Make all float-like strings integer-like
	all_voters_df['Precinct Name'] = all_voters_df['Precinct Name'].str.replace(pat = r'\.0', 
		repl = '')

	#Make all alphanumerics uppercase
	all_voters_df['Precinct Name'] = all_voters_df['Precinct Name'].str.upper()

	#strip leading zeroes in which there is an arbitrary number of them before another non-zero number
	all_voters_df['Precinct Name'] = all_voters_df['Precinct Name'].str.replace(pat = r'^0+\d', 
		repl = keep_numbers)


	all_voters_df['Precinct Name'] = all_voters_df['Precinct Name'].astype('category')

	#--------------------------------------------------------------------------
	#Cleaning Perm ZIP codes (they are currently strings)
		#NOTE: not cleaning Mailing ZIPs as they are all over the world, so 
		#I'd end up "cleaning" good data in some cases by applying these rules

	#Remove any decimal + trailing zeroes
	all_voters_df['Perm ZIP Code'] = all_voters_df['Perm ZIP Code'].str.replace(pat = r'(.0+)$', repl = '')


	#Remove all non-digit chars
	all_voters_df['Perm ZIP Code'] = all_voters_df['Perm ZIP Code'].str.replace(pat = r'(\D)', repl = '')


	#Less than 5 digits? Give it leading zeroes, to be safe
	all_voters_df['Perm ZIP Code'] = all_voters_df['Perm ZIP Code'].str.zfill(5)


	#More than 5 digits? Tableau can't geocode that, truncate to first 5
	all_voters_df['Perm ZIP Code'] = all_voters_df['Perm ZIP Code'].str.slice(stop = 5)



	#Now that the strings are clean, 
		#put back as categoricals for memory optimization
	all_voters_df = all_voters_df.astype({
		'Perm ZIP Code': 'category'})


	#--------------------------------------------------------------------------

	#There are no values in these, so setting to NaN
	all_voters_df[['Election Date', 'Election Type', 'Election Party', 
	'Election Voting Method']] = np.nan
	

	#Load geography crosswalk data for various aggregations and format to easily JOIN with voter rolls df
	geography_crosswalk = pd.read_csv(geo_crosswalk_file,
		dtype = {'PrecinctNameNoLeadingZeros': 'category'},
		na_values = ' ')

	#Select only the fields we'll need for analysis
	geography_crosswalk = geography_crosswalk[['state_county_code', 
	'PrecinctNameNoLeadingZeros', 'PrecinctId', 'state_house_district_latest', 
	'state_senate_district_latest', 'us_cong_district_latest']]

	#Give more useful human-readable names to columns and make consistent with other DataFrame
	geography_crosswalk.rename(columns = {'state_county_code': 'State County Code', 'PrecinctNameNoLeadingZeros': 'Precinct Name',
		'PrecinctId': 'Precinct ID', 
		'state_house_district_latest': 'State House District',
		'state_senate_district_latest': 'State Senate District',
		'us_cong_district_latest': 'US Congressional District'}, 
		inplace = True)

	#Make all alphanumerics uppercase
	geography_crosswalk['Precinct Name'] = geography_crosswalk['Precinct Name'].str.upper()

	#strip leading zeroes in which there is an arbitrary number of them before another non-zero number
	geography_crosswalk['Precinct Name'] = geography_crosswalk['Precinct Name'].str.replace(pat = r'^0+\d', 
		repl = keep_numbers)

	#Merging the geography crosswalk data with the voter rolls data
	output = pd.merge(left = all_voters_df, right = geography_crosswalk, 
		copy = False, how = 'left', 
		on = ['State County Code', 'Precinct Name'], indicator = True)

	for dtype in ['float', 'integer']:
		x = output.select_dtypes(include = [dtype]).columns
		output[x] = output[x].apply(pd.to_numeric, downcast = dtype)

	unmerged_count = len(output[output['_merge'] == 'left_only'])
	print(f"Number of unsuccessfully merged records = {unmerged_count}")
	print(f"Percentage of all records successfully merged with geographic crosswalk data = {unmerged_count / len(output)*100}%")

	return output


def calculate_missing(df, output_file = None):
	'''
	Calculates how many missing values there are for each field, as an 
	absolute count and as a percentage of all records

	Parameters
	----------
	df: pandas DataFrame of the format produced by import_voter_data()

	output_file: str. If not None, should be the filepath desired for 
					outputting the results as a CSV

	Returns
	-------
	missing: pandas DataFrame.
	'''

	missing = pd.DataFrame(df.isnull().sum()).rename(columns = {0: 'total missing'})
	missing['percent missing'] = missing['total missing'] / len(df)

	if output_file:
		missing.to_csv(output_file)

	return missing



def counts_by_geography(df, folder = None):
	'''
	Aggregates voter counts at different geographic levels, specifically:
		-Precincts
		-State House districts
		-State Senate districts
		-US Congressional districts

	and outputs the results to a different CSV file for each level

	Parameters
	----------
	df: pandas DataFrame of the format produced by import_voter_data()
	
	folder: str. Filepath of the desired folder for output files, 
			not the name(s) of the files. THIS METHOD WILL OVERWRITE ANY FILES 
			PREVIOUSLY GENERATED BY THIS METHOD IN THE SAME FOLDER.
			This string should end with '/'


	Returns
	-------
	dict of pandas DataFrames with keys 'Precinct ID', 'State House District', 
	'State Senate District', 'US Congressional District' and corresponding 
	DataFrames
	'''

	level_names = ['Precinct ID', 'State House District', 
	'State Senate District', 'US Congressional District']

	output_dict = {}

	for level in level_names:
		if folder[:-1] == '/':
			filepath = folder + 'voter_counts - ' + level + '.csv'
		else:
			filepath = folder + '/voter_counts - ' + level + '.csv'

		temp = df.groupby(level)['VUID (Voter ID)'].count().sort_values(ascending = False)

		temp.index = temp.index.astype('int')
		#level value of -1 is count of records missing a value 
		#in df for that level
		temp.loc[-1] = len(df) - temp.sum()
		temp = pd.DataFrame(temp).rename(columns = {'VUID (Voter ID)': 'Number of Voters'})

		#Save to files if folder directory is given.
		if folder: temp.to_csv(filepath)

		output_dict[level] = temp

	return output_dict


	def aggregate_data(df, agg_fields, output_file):
		'''
		Aggregates data on a specific field or list of fields such that it can then be shared publicly without concerns for privacy of the individual records.

		Parameters
		----------
		df: pandas DataFrame of the format produced by import_voter_data()

		agg_fields: list of str. Field names that should be part of the groupby aggregation. Note that it is recommended that fields with the most unique values be listed first and those with the least be listed last.

		output_file: str. The filepath desired for outputting the results as a CSV

		Returns
		-------
		Nothing, to avoid memory overflows. Use CSV file generated if needed.
		'''

		df.groupby(agg_fields).count().to_csv(filepath)